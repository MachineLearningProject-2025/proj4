# [MLP Project 4] Hull Tactical â€“ Market Prediction under Volatility Constraints

## ğŸ“Œ 1. Project Overview

| Detail                | Description                                                                                                                                                                                                 |
| :-------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Course**            | CS 53744 Machine Learning Project                                                                                                                                                                           |
| **Task**              | Time-series regression to predict **daily excess returns** of the S&P 500 and design a **volatility-constrained allocation strategy**.                                                                      |
| **Dataset**           | [Kaggle Competition â€“ Hull Tactical: Market Prediction](https://www.kaggle.com/competitions/hull-tactical-market-prediction)                                                                                |
| **Goal**              | (1) Predict `market_forward_excess_returns`, (2) map predictions to **daily weights w âˆˆ [0, 2]**, (3) satisfy **Ïƒ_strategy â‰¤ 1.2 Ã— Ïƒ_benchmark**, (4) maximize a **Modified Sharpe ratio**.                 |
| **Evaluation Metric** | Kaggle: Modified Sharpe ratio. Local: OOF RMSE & Correlation (for prediction quality) + **Sharpe, volatility ratio, cumulative return** (for strategy performance).                                         |
| **Final Model**       | **ElasticNet (PCA features) + LightGBM (raw FE)** blended (0.95 / 0.05) + **volatility-constrained allocation strategy**                                                                                    |
| **Baseline Models**   | Mean-prediction baseline, standalone ElasticNet, standalone LightGBM, standalone XGBoost (tested, excluded from final blend)                                                                                |
| **Key Insight**       | No model significantly beats the baseline in RMSE (consistent with EMH), but a **carefully regularized blend + mild leverage (k = 0.5)** achieves a **small Sharpe improvement under 120% volatility cap**. |

---

## ğŸ‘¥ 2. Team Information

| Role   | Name | GitHub ID     |
| :----- | :--- | :------------ |
| Member | ë°•ì›ê·œ  | `@keiro23`    |
| Member | ì´ìœ ì •  | `@yousrchive` |
| Member | ì •ìŠ¹í™˜  | `@whan0767`   |

---

## ğŸ† 3. Final Performance Summary

The final pipeline consists of:

1. **Rich feature engineering (lags, rolling stats, regimes, macro shocks, interactions)**
2. **Time-series cross-validation (walk-forward) without leakage**
3. **Model comparison & blending (ElasticNet + LightGBM)**
4. **Strategy evaluation under a volatility constraint (â‰¤ 120% of S&P 500)**

### 3.1 Prediction Performance (OOF, Time-Series CV)

Using 5-fold TimeSeriesSplit on the feature-engineered train set:

| Model              | RMSE (mean Â± std) | Corr (mean Â± std) | Comment                                     |
| :----------------- | :---------------- | :---------------- | :------------------------------------------ |
| **Baseline**       | â‰ˆ 0.0108 Â± 0.0027 | â‰ˆ 0.00            | Train-mean prediction                       |
| ElasticNet         | â‰ˆ 0.0111 Â± 0.0028 | â‰ˆ 0.03â€“0.04       | PCA(15) + ElasticNet                        |
| LightGBM           | â‰ˆ 0.0122 Â± 0.0025 | â‰ˆ 0.02â€“0.03       | Raw FE, tree-based boosting                 |
| XGBoost            | â‰ˆ 0.0124 Â± 0.0025 | â‰ˆ 0.03â€“0.04       | Slightly worse than ElasticNet / LightGBM   |
| **Blend (EN+LGB)** | â‰ˆ 0.0115          | â‰ˆ 0.035           | 0.95 ElasticNet + 0.05 LightGBM (RMSE-opt.) |

â†’ **Takeaway:** No single model clearly dominates the baseline; any predictability is extremely weak, consistent with EMH.

### 3.2 Strategy Performance (Vol-Constrained Allocation)

We convert blended predictions to daily weights:

* Standardize blended prediction: z_t
* Define weights: w_t = clip(1 + kÂ·z_t, 0, 2)
* Search k âˆˆ [0, 50] with step 0.5 under constraint Ïƒ_strategy â‰¤ 1.2 Ã— Ïƒ_benchmark

**Best k (under constraint)**: **k = 0.5**

| Metric                   | Benchmark (w = 1) | Blend Strategy (k = 0.5) |
| :----------------------- | :---------------- | :----------------------- |
| Mean daily excess return | â‰ˆ 0.000265        | â‰ˆ 0.000331               |
| Volatility ratio         | 1.0               | â‰ˆ 1.20 (capped)          |
| Annualized Sharpe        | â‰ˆ 0.378           | â‰ˆ 0.393                  |
| Final cumulative return  | â‰ˆ 0.400           | â‰ˆ 0.491                  |

**Interpretation:**
The performance gap is **small**, but under a strict volatility cap it indicates that **weak yet non-zero structure** in the feature space can be translated into a slight Sharpe improvement, which is conceptually consistent with EMHâ€™s â€œvery limited predictabilityâ€ view.

---

## âš™ï¸ 4. How to Reproduce Results

We separate the workflow into:

1. **Local / offline pipeline** (EDA, FE, TS-CV, backtesting, model export)
2. **Kaggle online inference** (evaluation API using `predict(test: pl.DataFrame)`)

### 4.1. Environment Setup & Dependencies

1. **Create & activate a virtual environment (local):**

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .\.venv\Scripts\activate  # Windows
```

2. **Install required packages:**

```bash
pip install -r requirements.txt
```

Key libraries:

* `numpy`, `pandas`, `polars`
* `scikit-learn`
* `lightgbm`, `xgboost` (optional)
* `matplotlib`

---

### 4.2. Data Preparation

1. Download `train.csv` and `test.csv` from the **Hull Tactical** Kaggle competition.
2. Place them inside the `data/` directory at the project root:

```text
Project4/
â””â”€â”€ data/
    â”œâ”€â”€ train.csv
    â””â”€â”€ test.csv  # mock test for structure reference
```

---

### 4.3. Local Pipeline: Feature Engineering, TS-CV & Backtesting

The offline pipeline does three things:

1. **Feature engineering on train** (`generate_FE_interaction_regime`)
2. **Time-series CV + OOF predictions** (`ts_cv_oof_predictions`)
3. **Blend optimization + strategy backtest** (`search_best_k_for_blend`)

Typical usage:

```bash
cd src

# 1) Run full training pipeline: FE + TS-CV + blending
python train_full_model.py

# 2) Run backtest & Sharpe evaluation using OOF predictions
python backtest.py
```

`train_full_model.py` (core steps):

* Load `train.csv`
* Drop leakage columns: `forward_returns`, `risk_free_rate` (keep only target `market_forward_excess_returns`)
* Apply **rich FE** (lags, rolling stats, volatility regimes, macro shocks, interactions, return shocks), always based on **past** information to avoid leakage
* Perform TimeSeriesSplit CV, training:

  * **ElasticNet(PCA)**: `StandardScaler` â†’ `PCA(15)` â†’ `ElasticNet`
  * **LightGBM**: raw FE
  * (Optionally) XGBoost
* Save:

  * `global_scaler.pkl`
  * `global_pca.pkl`
  * `elasticnet_model.pkl`
  * `lightgbm_model.txt`
  * `feature_list.json` (the final feature columns used for training)

`backtest.py`:

* Load OOF predictions and ground-truth target
* Construct blend: **0.95 ElasticNet + 0.05 LightGBM**
* Compute benchmark and strategy returns under k-grid search
* Enforce volatility â‰¤ 120% of benchmark
* Report:

  * mean returns, volatilities, Sharpe
  * cumulative return curves (Figure: `cumulative_returns_comparison.png`)

Outputs are saved under `models_fe_rich/` and `figures/`.

---

### 4.4. Kaggle Inference: Online predict() with Evaluation API

Kaggleâ€™s evaluation environment:

* No internet
* You receive **test batches** with:

  * Features M*, E*, I*, P*, V*, S*, MOM*, D*
  * `lagged_forward_returns`, `lagged_risk_free_rate`, `lagged_market_forward_excess_returns`
* You must implement:

```python
def predict(test: pl.DataFrame) -> float:
    ...
```

Core idea:

* Maintain a **buffer** of past rows in memory
* Use `lagged_market_forward_excess_returns` as the y_{tâˆ’1} equivalent
* Reproduce the **same FE logic as train**, but online & incremental
* Select `feature_list` columns in the correct order
* Apply:

  * `global_scaler` â†’ `global_pca` â†’ `elasticnet_model`
  * `lightgbm_model` on **raw FE**
  * Blend: `0.95 * pred_enet + 0.05 * pred_lgb`

Example (simplified) Kaggle-side script:

```python
import numpy as np
import polars as pl
import pickle
import json
import lightgbm as lgb

MODEL_PATH = "/kaggle/input/hull-tactical-dataset"

with open(f"{MODEL_PATH}/global_scaler.pkl", "rb") as f:
    scaler = pickle.load(f)

with open(f"{MODEL_PATH}/global_pca.pkl", "rb") as f:
    pca = pickle.load(f)

with open(f"{MODEL_PATH}/elasticnet_model.pkl", "rb") as f:
    enet = pickle.load(f)

lgb_model = lgb.Booster(model_file=f"{MODEL_PATH}/lightgbm_model.txt")

with open(f"{MODEL_PATH}/feature_list.json", "r") as f:
    feature_list = json.load(f)

BUFFER = { "rows": [] }

def make_test_FE(row: pl.DataFrame):

    global BUFFER
    BUFFER["rows"].append(row.to_dicts()[0])
    df = pl.from_dicts(BUFFER["rows"])

    # use lagged_market_forward_excess_returns as y_{t-1}
    df = df.with_columns([
        pl.col("lagged_market_forward_excess_returns").alias("y_lag1")
    ])

    # (1) rolling stats on y_lag1
    for w in [5,10,21,63]:
        df = df.with_columns([
            pl.col("y_lag1").rolling_mean(w).alias(f"roll_mean_{w}"),
            pl.col("y_lag1").rolling_std(w).alias(f"roll_std_{w}"),
        ])

    # (2) volatility regime
    df = df.with_columns([
        pl.col("y_lag1").rolling_std(21).alias("vol21"),
        pl.col("y_lag1").rolling_std(63).alias("vol63")
    ])
    df = df.with_columns([
        (pl.col("vol21") > pl.col("vol63")).cast(pl.Int8).alias("high_vol"),
        (pl.col("vol21") / (pl.col("vol63") + 1e-9)).alias("vol_slope"),
    ])

    # (3) macro shock on E*
    macro_cols = [c for c in df.columns if c.startswith("E")]
    for col in macro_cols:
        df = df.with_columns([
            ((pl.col(col) - pl.col(col).rolling_mean(63)) /
             (pl.col(col).rolling_std(63) + 1e-9)).alias(f"{col}_z")
        ])
        df = df.with_columns([
            (pl.col(f"{col}_z").abs() > 2).cast(pl.Int8).alias(f"{col}_shock")
        ])

    shock_cols = [c for c in df.columns if c.endswith("_shock")]
    if shock_cols:
        df = df.with_columns([
            sum([pl.col(c) for c in shock_cols]).alias("macro_shock_sum"),
            (pl.col("macro_shock_sum") >= 3).cast(pl.Int8).alias("macro_crisis"),
        ])
    else:
        df = df.with_columns([
            pl.lit(0).alias("macro_shock_sum"),
            pl.lit(0).cast(pl.Int8).alias("macro_crisis"),
        ])

    # (4) interaction: first few M and V
    m_cols = [c for c in df.columns if c.startswith("M")][:5]
    v_cols = [c for c in df.columns if c.startswith("V")][:5]
    for m in m_cols:
        for v in v_cols:
            df = df.with_columns((pl.col(m) * pl.col(v)).alias(f"{m}_x_{v}"))

    last = df.tail(1)

    # align with train-time features
    last = last.select([c for c in feature_list if c in last.columns])

    return last

def predict(test: pl.DataFrame) -> float:

    fe = make_test_FE(test)
    X = fe.to_numpy()

    X_scaled = scaler.transform(X)
    X_pca = pca.transform(X_scaled)
    pred_en = enet.predict(X_pca)

    pred_lgb = lgb_model.predict(X)

    pred = 0.95 * pred_en + 0.05 * pred_lgb

    return float(pred[0])
```

You then plug this into the provided evaluation template (`default_inference_server`) and submit the notebook.

---

## ğŸ“ 5. Project Directory Structure

```text
Project4/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv                  # mock structure (not used for scoring)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ feature_engineering.py    # generate_FE_interaction_regime, shared logic
â”‚   â”œâ”€â”€ train_full_model.py       # FE + TS-CV + model training + export
â”‚   â”œâ”€â”€ backtest.py               # blend & strategy evaluation, plots
â”‚   â”œâ”€â”€ kaggle_predict.py         # predict() demo for Kaggle evaluation API
â”‚   â””â”€â”€ utils.py                  # helper functions (metrics, plotting, etc.)
â”‚
â”œâ”€â”€ models_fe_rich/
â”‚   â”œâ”€â”€ global_scaler.pkl
â”‚   â”œâ”€â”€ global_pca.pkl
â”‚   â”œâ”€â”€ elasticnet_model.pkl
â”‚   â”œâ”€â”€ lightgbm_model.txt
â”‚   â”œâ”€â”€ feature_list.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA_and_StylizedFacts.ipynb
â”‚   â”œâ”€â”€ 02_TS_CV_Modeling.ipynb
â”‚   â”œâ”€â”€ 03_Blend_and_Strategy.ipynb
â”‚   â””â”€â”€ 04_Kaggle_Inference_Demo.ipynb
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ eda_returns_distribution.png
â”‚   â”œâ”€â”€ ts_cv_rmse.png
â”‚   â”œâ”€â”€ cumulative_returns_comparison.png
â”‚   â”œâ”€â”€ volatility_ratio_plot.png
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ Assignment4_TeamID_StudentID_Lastname_Firstname.pdf
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§© 6. Notes & Alignment with Course Requirements

* **Baseline vs Improved Models**

  * Baseline: mean predictor
  * Improved: ElasticNet, LightGBM, blended model

* **Feature Engineering & Validation Strategy**

  * Rich FE on lagged targets and macro variables
  * TimeSeriesSplit walk-forward CV to avoid leakage

* **Local Sharpe-variant & Volatility Plots**

  * Backtesting code generates cumulative return and volatility ratio plots

* **Kaggle Leaderboard**

  * Final submission created via the Kaggle inference pipeline above
  * Screenshot and commentary included in the PDF report

* **EMH Discussion**

  * Report links small Sharpe improvement and weak predictability back to EMH (approx. weak-form consistency).