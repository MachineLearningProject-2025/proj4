# ğŸš€ CS53744 Team Project 4 â€“ Hull Tactical Market Prediction

**Predicting S&P 500 Excess Returns & Designing a Volatility-Constrained Strategy**

This repository contains the full implementation, experiments, and report assets for **Team Project 4 (Hull Tactical Market Prediction)** of *CS 53744 Machine Learning*.
The goal of the project is to:

1. **Predict daily excess returns of the S&P 500 (market_forward_excess_returns)**
2. **Construct a daily allocation strategy w âˆˆ [0, 2]**
3. **Ensure portfolio volatility â‰¤ 120% of the benchmark**
4. **Maximize a Modified Sharpe Ratio**

This work integrates **time-series cross-validation, feature engineering, blending models, and backtesting**.
Kaggle competition link: [https://www.kaggle.com/competitions/hull-tactical-market-prediction](https://www.kaggle.com/competitions/hull-tactical-market-prediction)

---

## ğŸ“Œ Repository Structure

```
â”œâ”€â”€ data/ 
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv (mock)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA_and_StylizedFacts.ipynb
â”‚   â”œâ”€â”€ 02_TS_CV_Modeling.ipynb
â”‚   â”œâ”€â”€ 03_Blend_and_Strategy.ipynb
â”‚   â”œâ”€â”€ 04_Kaggle_Inference_Demo.ipynb
â”‚
â”œâ”€â”€ models_fe_rich/
â”‚   â”œâ”€â”€ global_scaler.pkl
â”‚   â”œâ”€â”€ global_pca.pkl
â”‚   â”œâ”€â”€ 
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ train_full_model.py
â”‚   â”œâ”€â”€ backtest.py
â”‚   â”œâ”€â”€ kaggle_predict.py
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ eda_returns_distribution.png
â”‚   â”œâ”€â”€ ts_cv_rmse.png
â”‚   â”œâ”€â”€ cumulative_returns_comparison.png
â”‚   â”œâ”€â”€ volatility_ratio_plot.png
â”‚
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ Assignment4_TeamID_StudentID_Lastname_Firstname.pdf
â”‚
â”œâ”€â”€ requirements.txt
â”‚
â””â”€â”€ README.md
```

---

# 1. ğŸ§  Problem Overview

The Hull Tactical Market Prediction challenge requires predicting the next-day **excess return**:

[
y_t = \text{forward_returns}_t - \text{risk_free_rate}_t
]

and turning predictions into **allocation weights**:

[
w_t \in [0, 2], \qquad
\text{Volatility}(w_t y_t) \le 1.2 \times \text{Volatility}(y_t)
]

The evaluation metric is a **Modified Sharpe Ratio**, penalizing excessive volatility or poor performance.

We frame the task as a **no-leakage time-series ML problem**, then convert predictions into a constrained investment strategy.

---

# 2. ğŸ“Š Dataset Description & EMH Perspective

The dataset contains ~9,000 daily observations and 98 market-related features:

* **M***: market dynamics
* **E***: macroeconomic variables
* **V***: volatility features
* **I***: interest rate features
* **P***: valuation features
* **S***: sentiment
* **MOM***: momentum
* **D***: binary indicators
* **Forward returns / RF rate / excess returns** (train only)

Empirical findings (stylized facts):

* Daily excess returns â‰ˆ **zero mean + high noise**
* **Fat-tailed**, **leptokurtic** distribution
* **Almost no autocorrelation** in returns
* Strong **latent factor structure** (multicollinearity)
  â†’ Consistent with the **Efficient Market Hypothesis (EMH)**:
  short-term direction is extremely hard to predict, but *market states (volatility/regime/macro shock)* may show limited predictability.

---

# 3. âš™ï¸ Feature Engineering

To capture market structure (rather than direction), we designed a rich FE pipeline:

### âœ” Lag Features

* y_{tâˆ’1}, y_{tâˆ’2}, y_{tâˆ’5}, y_{tâˆ’10}, y_{tâˆ’21}, y_{tâˆ’63}

### âœ” Rolling Statistics (no leakage; always based on lagged values)

* rolling mean / std / min / max
* rolling z-scores
* vol21, vol63, high_vol flag, vol_slope
* crisis regime (rolling quantile)

### âœ” Macro Shock Indicators (E*)

* rolling z-scores
* shock flags (>|2Ïƒ|)
* macro_shock_sum
* macro_crisis = 1 if â‰¥3 macro shocks occur simultaneously

### âœ” Interaction Features

* momentum Ã— volatility : M_i Ã— V_j
* macro spreads (E2â€“E11, E7â€“E12, â€¦)

### âœ” Return Shock Indicator

* |y_{tâˆ’1}| > 2 Ã— rolling std

### âœ” Train vs. Predict Mode

* Train uses target.shift(1)
* Kaggle online inference uses lagged_market_forward_excess_returns (provided by test.csv)

This FE significantly increased the modelâ€™s ability to detect *regimes* rather than â€œdirectionsâ€.

---

# 4. ğŸ¤– Modeling & Time-Series Validation

We train the following models:

* **ElasticNet (with StandardScaler + PCA)**
* **LightGBM**
* **(XGBoost tested; excluded in final blend)**

### Time-Series Cross-Validation

We use **walk-forward TS-CV (5 splits)** to avoid leakage.

### Results Summary

| Model           | RMSE (OOF)     | Corr (OOF) |
| --------------- | -------------- | ---------- |
| Baseline (mean) | ~0.0108        | ~0.00      |
| ElasticNet      | ~0.0111        | ~0.03â€“0.04 |
| LightGBM        | ~0.0122        | ~0.01â€“0.02 |
| XGBoost         | Underperformed | â€”          |

â¡ **Small but consistent predictive signal**, aligned with EMH expectations.

---

# 5. ğŸ”— Model Blending

Grid search over weights (0.05 step):

### **Best Blend (RMSE-optimal)**

* **ElasticNet: 0.95**
* **LightGBM: 0.05**
* XGBoost: 0.00

This blend provides the most stable OOF performance.

---

# 6. ğŸ“ˆ Strategy Construction

Weights generated as:

[
z_t = \frac{\hat{y}_t - \mu}{\sigma}, \quad
w_t = \text{clip}(1 + k z_t, 0, 2)
]

We search k âˆˆ [0, 50]:

### **Best k = 0.5

(under volatility â‰¤ 120%)**

### Final Strategy vs Benchmark

| Metric                  | Benchmark | Strategy  |
| ----------------------- | --------- | --------- |
| Annualized Return       | ~0.000265 | ~0.000331 |
| Annualized Vol          | same      | Ã—1.20     |
| Sharpe                  | **0.378** | **0.393** |
| Final Cumulative Return | **0.400** | **0.491** |

The improvement is small but statistically meaningful, given EMH constraints.

---

# 7. ğŸ§ª Kaggle Submission Pipeline

We export:

* 

Kaggle `predict()` implements *online feature engineering* using the provided:

* `lagged_forward_returns`
* `lagged_risk_free_rate`
* `lagged_market_forward_excess_returns`

Buffering & rolling logic reconstructs train-time FE without leakage.

---

# 8. ğŸ“ Final Deliverables

Included in this repo:

* **Prediction file (test.csv)**
* **Kaggle Notebook** (with modeling, FE, backtesting, plots)
* **4-page Report (PDF)**
* **GitHub repo with code, models, figures**
* **Leaderboard screenshot**

---

# 9. ğŸ” Limitations & Future Work

* Deep sequence models (LSTM/Transformer) not included
* Early test timesteps have incomplete FE
* PCA may obscure macro interpretability
* Additional external datasets could help (news sentiment, VIX, macro releases)

---

# 10. ğŸ“š How to Reproduce

```
git clone <this-repo>
cd project

pip install -r requirements.txt

python src/train_full_model.py     # Re-train FE + models
python src/backtest.py             # Evaluate strategy locally
```

To submit to Kaggle:

1. Upload `models_fe_rich/` as a Kaggle Dataset
2. Attach dataset to your Notebook
3. Run `kaggle_predict.py` with the evaluation API

---

# 11. ğŸ™Œ Team & Acknowledgements

This project was completed as part of **CS 53744 Machine Learning**
Instructor: **Prof. Jongmin Lee**
Team Members: *[Insert Names]*

We thank Kaggle and Hull Tactical for providing the research environment.